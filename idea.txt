************* System Design 1 **********************
UI (Frontend), deployed with the Python FastAPI backend, launches with automatic OAuth, and presents a ChatGPT-like interface. The user can create new chats, view/select MCP servers, and connect to them to fetch tools dynamically.
FastAPI Backend:
Socket.IO Manager for real-time chat.
Per-user Handlers: Each user has their own LLM Handler (builds and manages a Graph using LangGraph/LangChain with GPT-4 and tools specific to the selected MCP server) and MCP Handler (manages the connection to MCP server and tool retrieval).
Database (app.db): Stores users, chats (with thread_id for history), MCP servers, etc.
Checkpointer: Persists conversation history per chat/thread (file-based SQLite).
LangGraph / LangChain:
Each user’s LLM Handler builds/compiles the conversation graph using the retrieved tools, GPT-4 as the model, and plug-in modules.
Graph logic: LLM → Tool → Human Approval → if approved: Tools, else: LLM → Humanized Response → End.
External MCP Servers (provide tools listing & execution).
OAuth Provider (User login).
GPT-4 API.
Architecture Diagram
This diagram groups components logically, shows deployment, dynamic tool fetching per-user, real-time communication, and external service dependencies.

Key Interaction Sequence Diagram
Focus: New user session, OAuth, new chat creation, MCP connection, tool loading, dynamic graph build, and chat invocation.

Summary of What’s Improved in this Architecture Design:

All per-user state and built graphs handled server-side, mapped by user/session (RESTful and scalable!).
Dynamic tool fetching/binding for each chat/linked MCP, supporting personal “agent” logic for each user context.
Real-time chat and per-thread history managed and checkpointed.
Clean separation of OAuth, DB, session-handlers, Chat/Checkpointer logic, LLM/graph engines, and tool sources.


*************************** system design v2 *****************************
Summary: The system comprises a UI (with automatic OAuth), a FastAPI backend managing per-user session handlers for MCP and LLM (LangGraph/LangChain+GPT-4), a dynamic tool plumbing layer per-user, real-time Socket.IO for chat, and persistence via SQLite. Each chat allows fetching tools dynamically from user-selected MCP servers, compiles a custom agent with tools using a graph per user, and real-time chat is managed with per-thread checkpointing.

Improved System Architecture Prompt (for Documentation, PRs, or Stakeholders)
This system is a multi-user chat platform with dynamic agent compilation. The frontend launches with automatic OAuth, then enables users to start chat sessions, dynamically link to any external MCP tool server, and create custom chat agents on the fly for each user and chat.

Key Technical Points:

Per-user LLM and MCP handlers are managed in FastAPI using a user-id mapped registry.
Tools from MCP servers are dynamically fetched per session and bound during LangGraph agent compile time (using GPT-4/LangChain).
Each Chat has a separate thread/checkpointer (SQLite), chat records, and allows ad-hoc tool selection.
State is strictly per-user and does not leak; all logic is isolated by user identity/session.
Upgrades are easy: swap out GPT-4, add new tool server types, replace DB/checkpoint backend, or enhance the chat graph.

**************************** SYSTEM DESIGN V3 ***************************************
## System Overview

The system is a sophisticated, session-based conversational AI platform. It features a Python backend using FastAPI and a dynamic web UI. The core of the system provides a chat interface where each user's conversational agent can be dynamically equipped with different tools from various sources. The architecture is designed to be modular and stateful on a per-user basis, with in-memory handlers managing the logic for each active user.

## Core Components

UI (Web Interface): A standard chat interface where users can manage multiple conversations. It authenticates the user, displays a list of available tool servers (MCP Servers), and communicates with the backend via both HTTP REST endpoints for setup and WebSockets (Socket.IO) for real-time chat messages.

FastAPI Backend: The central application server.

HTTP Endpoints: Manages user authentication, session initialization (/initialize), fetching server lists (/servers), and connecting to tool servers (/connect). It's also responsible for creating new chat sessions and their associated thread_id in the database.

Socket.IO Manager: Handles persistent, authenticated WebSocket connections for real-time, bidirectional chat communication. It receives messages from the UI and routes them to the appropriate user's handler.

Handler Managers (In-Memory Singletons): Two manager classes (LLMHandlerManager, MCPHandlerManager) that maintain Python dictionaries. These dictionaries map a user_id to an instance of a handler, ensuring each user has their own dedicated object to manage their state.

Per-User MCP Handler: An object responsible for managing connections to the Model Context Protocol (MCP) Servers. When a user connects to a server, this handler's client fetches the list of available tools from that specific server.

Per-User LLM Handler: This object is the primary controller for a user's conversation. It contains the compiled LangGraph instance. Its key responsibility is to receive requests from the API/Socket layers and invoke the graph. It also triggers the recompilation of the graph when new tools are provided by the MCP Handler.

LangGraph Engine: This is the heart of the AI logic, contained within the LLM Handler.

Dynamic Compilation: The graph is compiled on a per-user basis. It's initially compiled without tools and then re-compiled with new tools when a user connects to an MCP Server.

Execution Flow: The graph's defined process is: Call LLM -> Check for Tool Call -> Human Approval Step -> (If Approved) -> Execute Tool -> Loop to LLM with result; (If Rejected) -> Call LLM for a humanized response -> End.

State Management: It uses a thread_id (provided with each message) to maintain conversation history via a Checkpointer.

Data Stores:

Application DB (SQLite): A database that stores persistent data such as user accounts, chat metadata (including the crucial thread_id for each conversation), and the list of available MCP Servers.

Checkpointer (File-based SQLite): Used exclusively by LangGraph to save and load the state of each conversation thread, enabling memory.

Internal Services:

Internal MCP Server: An internal service that exposes a set of tools that the agent can use.

GPT-4.1: A self-hosted large language model running on the organization's private AWS cloud.

Azure AD & OAuth Provider: Internal services used for user authentication and for securing the internal GPT-4.1 endpoint.

## Key Workflows

Initialization & Tool Loading:

The user logs in via OAuth. An initialize API call creates the user's LLM Handler and MCP Handler in memory.

The UI fetches and displays a list of available MCP Servers.

The user clicks "Connect" on a server. A connect API call triggers the MCP Handler to fetch tools from that server.

The MCP Handler then passes these tools to the LLM Handler, which re-compiles its internal LangGraph instance, making the new tools available for the agent.

Chat Processing:

The user starts a "New Chat," which calls an HTTP endpoint to create a record in the Application DB and generates a unique thread_id. The UI receives and stores this thread_id.

For every message, the UI sends the message text and the corresponding thread_id over the Socket.IO connection.

The backend's Socket.IO Manager receives the payload, identifies the user, and retrieves their specific LLM Handler instance.

It invokes the handler's LangGraph with the message and the thread_id. The graph executes its defined logic (calling the LLM, tools, etc.), using the Checkpointer and thread_id to maintain conversation history.

The final response is streamed back to the UI over the WebSocket.

